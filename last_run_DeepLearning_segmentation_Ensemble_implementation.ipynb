{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punith-naga/ft/blob/main/last_run_DeepLearning_segmentation_Ensemble_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XLtYlbRFan6K",
      "metadata": {
        "id": "XLtYlbRFan6K"
      },
      "source": [
        "\n",
        "\n",
        "**Step 1: Import Libraries:**This section imports all necessary Python libraries for medical image processing, deep learning model building, and visualization. Key libraries include nibabel for NIfTI image handling, torch for deep learning, numpy for numerical operations, matplotlib for plotting, and segmentation_models_pytorch (smp) which provides implementations of popular segmentation architectures like UNet and DeepLabV3+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9gCSw-ihESEQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gCSw-ihESEQ",
        "outputId": "22757e72-ee4f-4880-b399-1ae03b027ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.13.0.92)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.6.0)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.3)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (26.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.7.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.0.24)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.24.0+cu128)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24->segmentation_models_pytorch) (8.3.1)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation_models_pytorch\n",
            "Successfully installed segmentation_models_pytorch-0.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Imports done\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# %%\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!pip install albumentations\n",
        "!pip install nibabel\n",
        "!pip install opencv-python\n",
        "!pip install segmentation_models_pytorch # Includes UNet, DeepLabV3+, etc.\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow # Keep if you need it for other parts, but not for these models\n",
        "\n",
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset\n",
        "import tensorflow as tf # Keep if needed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "\n",
        "# Import models from segmentation_models_pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "print(f\"Imports done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_PwmKrSAakgU",
      "metadata": {
        "id": "_PwmKrSAakgU"
      },
      "source": [
        "**Step 2: Helper Functions and DataLoader:** This part of the notebook defines various helper functions crucial for the entire pipeline. It includes dice_loss and dice_score for metric calculation, visualization functions like plot_sample_colored and plot_sample_separate_modalities (adapted for 3D slices), and data manipulation utilities such as pad_or_crop_to_shape and safe_unsqueeze_mask. Most importantly, it defines the ISLESDataset3D class for loading and preprocessing 3D NIfTI medical images (DWI, ADC, and masks) and a pad_collate function to handle batching of variable-sized 3D volumes. It also introduces ensemble_predict and ensemble_predict_slice_by_slice functions for combining predictions from multiple models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1yptYyujZmWQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yptYyujZmWQ",
        "outputId": "674fe0ec-4b32-4c64-94f3-115dd4f71471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Warning: LOCAL_DOWNLOAD_ROOT not found. Falling back to default path.\n",
            "Using : cuda\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "# 1. Set up paths and parameters\n",
        "\n",
        "# Update DATA_DIR to use the path from kagglehub download.\n",
        "# Assuming the downloaded dataset structure is LOCAL_DOWNLOAD_ROOT/isles-2022\n",
        "# We need to construct the path to the 'isles-2022' subfolder within the downloaded root.\n",
        "# Make sure `LOCAL_DOWNLOAD_ROOT` is defined from the previous cell.\n",
        "# The actual path might vary, so adjust based on the exact structure after download.\n",
        "\n",
        "# Assuming the dataset structure is like: /kaggle/input/orvile/isles-2022-brain-stoke-dataset/isles-2022\n",
        "# Let's dynamically create this path.\n",
        "if 'LOCAL_DOWNLOAD_ROOT' in globals():\n",
        "    DATA_DIR = os.path.join(LOCAL_DOWNLOAD_ROOT, \"isles-2022\")\n",
        "    print(f\"Updated DATA_DIR to: {DATA_DIR}\")\n",
        "else:\n",
        "    print(\"Warning: LOCAL_DOWNLOAD_ROOT not found. Falling back to default path.\")\n",
        "    DATA_DIR = \"/content/drive/My Drive/ISLES\" # Fallback if kagglehub.download wasn't executed\n",
        "\n",
        "N_MODELS = 1 # Train 3 models for the ensemble\n",
        "EPOCHS = 10 # Reduce epochs for testing, increase for proper training\n",
        "BATCH_SIZE = 2 # Reduce batch size due to model complexity and 3D data\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_paths_dict = {}\n",
        "\n",
        "print(\"Using :\", DEVICE)\n",
        "# ...existing code...\n",
        "\n",
        "TARGET_SHAPE = (96, 96, 96)  # Increased target shape to prevent 1x1 feature maps after downsampling\n",
        "\n",
        "\n",
        "# Helper functions (Your existing code)\n",
        "def dice_loss(pred, target, smooth=1.):\n",
        "    pred = pred.reshape(-1)\n",
        "    target = target.reshape(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
        "\n",
        "def dice_score(pred, target, smooth=1.):\n",
        "    pred = pred.reshape(-1)\n",
        "    target = target.reshape(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "\n",
        "def plot_sample(x, y, pred, channel=0):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Input (selected channel)\")\n",
        "    plt.imshow(x[channel].cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.imshow(y.squeeze().cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.imshow(pred.squeeze().cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def safe_unsqueeze_mask(y):\n",
        "    # Ensure mask is [B, 1, D, H, W]\n",
        "    if y.ndim == 4:\n",
        "        y = y.unsqueeze(1)\n",
        "    elif y.ndim == 5 and y.shape[1] != 1:\n",
        "        # If mask has extra channels, take the first\n",
        "        y = y[:, :1, ...]\n",
        "    elif y.ndim < 4:\n",
        "        raise ValueError(f\"Mask shape too small: {y.shape}\")\n",
        "    return y\n",
        "\n",
        "# ...existing code...\n",
        "def pad_or_crop_to_shape_2d(x, target_shape_2d):\n",
        "    # x: [B, C, H, W]\n",
        "    _, _, H, W = x.shape\n",
        "    tH, tW = target_shape_2d\n",
        "    # Pad\n",
        "    pad_h_before = (tH - H) // 2\n",
        "    pad_h_after = tH - H - pad_h_before\n",
        "    pad_w_before = (tW - W) // 2\n",
        "    pad_w_after = tW - W - pad_w_before\n",
        "\n",
        "    # Padding order for F.pad is (W_left, W_right, H_top, H_bottom) for 4D\n",
        "    x = F.pad(x, [pad_w_before, pad_w_after, pad_h_before, pad_h_after])\n",
        "\n",
        "    # Crop (only if needed, padding should handle this)\n",
        "    x = x[:, :, :tH, :tW]\n",
        "    return x\n",
        "# Make sure to include the pad_or_crop_to_shape helper function:\n",
        "def pad_or_crop_to_shape(x, target_shape):\n",
        "    # x: [B, C, D, H, W]\n",
        "    _, _, D, H, W = x.shape\n",
        "    tD, tH, tW = target_shape\n",
        "    # Pad\n",
        "    pad_d_before = (tD - D) // 2\n",
        "    pad_d_after = tD - D - pad_d_before\n",
        "    pad_h_before = (tH - H) // 2\n",
        "    pad_h_after = tH - H - pad_h_before\n",
        "    pad_w_before = (tW - W) // 2\n",
        "    pad_w_after = tW - W - pad_w_before\n",
        "\n",
        "    # Padding order is (W_left, W_right, H_top, H_bottom, D_front, D_back) for 5D\n",
        "    x = F.pad(x, [pad_w_before, pad_w_after, pad_h_before, pad_h_after, pad_d_before, pad_d_after])\n",
        "\n",
        "    # Crop (only if needed, padding should handle this)\n",
        "    x = x[:, :, :tD, :tH, :tW]\n",
        "    return x\n",
        "\n",
        "\n",
        "# Evaluation Metrics (Add AVD and F1 score functions here - if you have them)\n",
        "def absolute_volume_difference(pred, target):\n",
        "    # Implement AVD calculation\n",
        "    # Example (assuming pred and target are binary tensors):\n",
        "    pred_volume = torch.sum(pred)\n",
        "    target_volume = torch.sum(target)\n",
        "    if target_volume == 0: # Avoid division by zero\n",
        "        return float('inf') if pred_volume > 0 else 0.0\n",
        "    avd = torch.abs(pred_volume - target_volume) / target_volume * 100\n",
        "    return avd.item()\n",
        "\n",
        "\n",
        "def lesion_wise_f1_score(pred, target):\n",
        "    # Implement lesion-wise F1 score calculation\n",
        "    # This requires identifying individual lesions. A common approach is\n",
        "    # to use connected components analysis on the ground truth and predictions. Therefore requires libraries like SciPy or OpenCv.\n",
        "    # For a simple pixel-wise F1, you can calculate that.\n",
        "\n",
        "    # Simple pixel-wise F1:\n",
        "    pred = pred.reshape(-1).bool()\n",
        "    target = target.reshape(-1).bool()\n",
        "\n",
        "    tp = (pred & target).sum().item()\n",
        "    fp = (pred & ~target).sum().item()\n",
        "    fn = (~pred & target).sum().item()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def pad_or_crop_5d(x, target_shape):\n",
        "    # x: [B, C, D, H, W] or [B, 1, D, H, W]\n",
        "    _, _, D, H, W = x.shape\n",
        "    tD, tH, tW = target_shape\n",
        "    # Pad\n",
        "    pad_d = max(tD - D, 0)\n",
        "    pad_h = max(tH - H, 0)\n",
        "    pad_w = max(tW - W, 0)\n",
        "    x = F.pad(x, [0, pad_w, 0, pad_h, 0, pad_d])\n",
        "    # Crop\n",
        "    x = x[:, :, :tD, :tH, :tW]\n",
        "    return x\n",
        "\n",
        "\n",
        "# ...existing code...\n",
        "\n",
        "def safe_pad_or_crop(x, target_shape):\n",
        "    # Accepts [B, C, D, H, W] or [B, D, H, W] or [C, D, H, W]\n",
        "    if x.ndim == 4:\n",
        "        x = x.unsqueeze(0)  # Add batch dim if missing\n",
        "    if x.ndim == 5:\n",
        "        _, _, D, H, W = x.shape\n",
        "        tD, tH, tW = target_shape\n",
        "        pad_d = max(tD - D, 0)\n",
        "        pad_h = max(tH - H, 0)\n",
        "        pad_w = max(tW - W, 0)\n",
        "        x = F.pad(x, [0, pad_w, 0, pad_h, 0, pad_d])\n",
        "        x = x[:, :, :tD, :tH, :tW]\n",
        "    else:\n",
        "        raise ValueError(f\"Input shape not supported: {x.shape}\")\n",
        "    return x\n",
        "\n",
        "# %%\n",
        "class ISLESDataset3D(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.samples = []\n",
        "        print(f\"entering 3D samples\")\n",
        "        mask_root = os.path.join(root_dir, \"derivatives\")\n",
        "        for subject in os.listdir(root_dir):\n",
        "            if subject.startswith(\"sub-\"):\n",
        "                ses_dir = os.path.join(root_dir, subject, \"ses-0001\")\n",
        "                if os.path.exists(ses_dir):\n",
        "                    dwi_dir = os.path.join(ses_dir, \"dwi\")\n",
        "                    anat_dir = os.path.join(ses_dir, \"anat\")\n",
        "\n",
        "                    # Look for both dwi and adc files in the dwi directory\n",
        "                    dwi_files = [f for f in os.listdir(dwi_dir) if f.endswith(\"_dwi.nii.gz\")]\n",
        "                    adc_files = [f for f in os.listdir(dwi_dir) if f.endswith(\"_adc.nii.gz\")]\n",
        "\n",
        "                    flair_paths = [f for f in os.listdir(anat_dir) if f.endswith(\"_FLAIR.nii.gz\")]\n",
        "\n",
        "                    mask_dir = os.path.join(mask_root, subject, \"ses-0001\")\n",
        "                    mask_path = []\n",
        "                    if os.path.exists(mask_dir):\n",
        "                        mask_path = [f for f in os.listdir(mask_dir) if f.endswith(\".nii.gz\")]\n",
        "\n",
        "                    # Ensure all required files are found\n",
        "                    if dwi_files and adc_files and flair_paths and mask_path:\n",
        "                         # For this ensemble, we'll use DWI, ADC, and FLAIR as inputs (3 channels)\n",
        "                        self.samples.append({\n",
        "                            \"dwi\": os.path.join(dwi_dir, dwi_files[0]),\n",
        "                            \"adc\": os.path.join(dwi_dir, adc_files[0]), # Add ADC path\n",
        "                            \"flair\": os.path.join(anat_dir, flair_paths[0]), # Add FLAIR path\n",
        "                            \"mask\": os.path.join(mask_dir, mask_path[0])\n",
        "                        })\n",
        "        print(f\"Total 3D samples: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        dwi = self.load_nifti(sample[\"dwi\"])    # [H, W, D]\n",
        "        adc = self.load_nifti(sample[\"adc\"])    # [H, W, D] - Load ADC\n",
        "        flair = self.load_nifti(sample[\"flair\"]) # [H, W, D] - Load FLAIR\n",
        "\n",
        "        # Crop all to the minimum shape\n",
        "        min_shape = np.minimum.reduce([dwi.shape, adc.shape, flair.shape])\n",
        "        dwi_cropped = dwi[:min_shape[0], :min_shape[1], :min_shape[2]]\n",
        "        adc_cropped = adc[:min_shape[0], :min_shape[1], :min_shape[2]]\n",
        "        flair_cropped = flair[:min_shape[0], :min_shape[1], :min_shape[2]]\n",
        "\n",
        "\n",
        "        x = np.stack([dwi_cropped, adc_cropped, flair_cropped], axis=0)  # Stack DWI, ADC, FLAIR [3, H, W, D]\n",
        "        y = self.load_nifti(sample[\"mask\"])\n",
        "        y = y[:min_shape[0], :min_shape[1], :min_shape[2]]  # Crop mask to match\n",
        "\n",
        "\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_nifti(path):\n",
        "        return np.asarray(nib.load(path).get_fdata(), dtype=np.float32)\n",
        "\n",
        "# %%\n",
        "\n",
        "def load_ensemble(model_paths_dict, device):\n",
        "    models = []\n",
        "    for model_name, paths in model_paths_dict.items():\n",
        "         # Get config to recreate the model structure\n",
        "        arch_name = model_name.split('_')[0] # Extract architecture name\n",
        "        encoder_name = '_'.join(model_name.split('_')[1:]) # Extract encoder name\n",
        "\n",
        "        for path in paths:\n",
        "            try:\n",
        "                 # Recreate the model structure\n",
        "                 model = get_smp_model(\n",
        "                    arch=arch_name,\n",
        "                    encoder_name=encoder_name,\n",
        "                    in_channels=3, # DWI, ADC, and FLAIR\n",
        "                    out_classes=1  # Binary segmentation\n",
        "                 ).to(device)\n",
        "\n",
        "                 # Load the state dictionary\n",
        "                 model.load_state_dict(torch.load(path, map_location=device))\n",
        "                 model.eval() # Set model to evaluation mode\n",
        "                 models.append(model)\n",
        "                 print(f\"Loaded {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {path}: {e}\")\n",
        "                # Decide how to handle loading errors (e.g., skip the model)\n",
        "                continue\n",
        "    return models\n",
        "\n",
        "\n",
        "# Helper functions for visualization (adapt if needed for 3D slices)\n",
        "def plot_sample_colored(x, y, pred, channel=0, slice_idx=None):\n",
        "    # Adapting for 3D: need to select a slice\n",
        "    if x.ndim == 5: # [B, C, D, H, W]\n",
        "        if slice_idx is None:\n",
        "             slice_idx = x.shape[2] // 2 # Use middle slice if not specified\n",
        "        img_slice = x[0, channel, slice_idx].cpu().squeeze() # Assuming batch size 1 for visualization\n",
        "        y_slice = y[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, 1 channel for mask\n",
        "        pred_slice = pred[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, 1 channel for prediction\n",
        "    elif x.ndim == 4: # Assume [C, D, H, W] for single sample visualization\n",
        "         if slice_idx is None:\n",
        "             slice_idx = x.shape[1] // 2\n",
        "         img_slice = x[channel, slice_idx].cpu().squeeze()\n",
        "         y_slice = y[0, slice_idx].cpu().squeeze() # Assuming 1 channel for mask\n",
        "         pred_slice = pred[0, slice_idx].cpu().squeeze() # Assuming 1 channel for prediction\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported input shape for plotting: {x.shape}\")\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    img = img_slice\n",
        "    img = (img - img.min()) / (img.max() - img.min() + 1e-8) # Normalize\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.imshow(y_slice, cmap='Greens', alpha=0.3)      # Ground truth in green\n",
        "    ax.imshow(pred_slice, cmap='Reds', alpha=0.3)     # Prediction in red\n",
        "    ax.set_title(\"Input + GT (green) + Pred (red)\")\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_sample_separate_modalities(x, y, pred, slice_idx=None):\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        if x.ndim == 5: # [B, C, D, H, W]\n",
        "            if slice_idx is None:\n",
        "                 slice_idx = x.shape[2] // 2\n",
        "            dwi_img_slice = x[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, DWI channel 0\n",
        "            adc_img_slice = x[0, 1, slice_idx].cpu().squeeze() # Assuming batch size 1, ADC channel 1\n",
        "            flair_img_slice = x[0, 2, slice_idx].cpu().squeeze() # Assuming batch size 1, FLAIR channel 2\n",
        "            y_slice = y[0, 0, slice_idx].cpu().squeeze()\n",
        "            pred_slice = pred[0, 0, slice_idx].cpu().squeeze()\n",
        "        elif x.ndim == 4: # Assume [C, D, H, W] for single sample visualization\n",
        "             if slice_idx is None:\n",
        "                 slice_idx = x.shape[1] // 2\n",
        "             dwi_img_slice = x[0, slice_idx].cpu().squeeze()\n",
        "             adc_img_slice = x[1, slice_idx].cpu().squeeze()\n",
        "             flair_img_slice = x[2, slice_idx].cpu().squeeze() # FLAIR channel 2\n",
        "             y_slice = y[0, slice_idx].cpu().squeeze()\n",
        "             pred_slice = pred[0, slice_idx].cpu().squeeze()\n",
        "        else:\n",
        "             raise ValueError(f\"Unsupported input shape for plotting: {x.shape}\")\n",
        "\n",
        "        print(f\"DEBUG: y_slice unique values: {torch.unique(y_slice)}\")\n",
        "        print(f\"DEBUG: y_slice sum: {y_slice.sum()}\")\n",
        "        print(f\"DEBUG: flair_img_slice min: {flair_img_slice.min()}, max: {flair_img_slice.max()}, mean: {flair_img_slice.mean()}, std: {flair_img_slice.std()}\")\n",
        "\n",
        "        # Normalize for better visualization\n",
        "        dwi_img_slice = (dwi_img_slice - dwi_img_slice.min()) / (dwi_img_slice.max() - dwi_img_slice.min() + 1e-8)\n",
        "        adc_img_slice = (adc_img_slice - adc_img_slice.min()) / (adc_img_slice.max() - adc_img_slice.min() + 1e-8)\n",
        "\n",
        "        # Conditional normalization for FLAIR to handle blank images gracefully\n",
        "        if flair_img_slice.max() == flair_img_slice.min():\n",
        "            flair_img_slice_normalized = torch.zeros_like(flair_img_slice) # If all values are same, make it black\n",
        "            print(\"DEBUG: FLAIR image is uniform (all same value), normalized to black.\")\n",
        "        else:\n",
        "            flair_img_slice_normalized = (flair_img_slice - flair_img_slice.min()) / (flair_img_slice.max() - flair_img_slice.min() + 1e-8)\n",
        "        print(f\"DEBUG: flair_img_slice_normalized min: {flair_img_slice_normalized.min()}, max: {flair_img_slice_normalized.max()}, mean: {flair_img_slice_normalized.mean()}, std: {flair_img_slice_normalized.std()}\")\n",
        "\n",
        "\n",
        "        # Plot DWI Input\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray')\n",
        "        plt.title(f\"Input (DWI) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ADC Input\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(adc_img_slice, cmap='gray')\n",
        "        plt.title(f\"Input (ADC) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot FLAIR Input\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(flair_img_slice_normalized, cmap='gray')\n",
        "        plt.title(f\"Input (FLAIR) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot Ground Truth\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray') # Optionally show DWI in background\n",
        "        plt.imshow(y_slice, cmap='Greens', alpha=0.7) # Increased alpha for better visibility\n",
        "        plt.title(f\"Ground Truth Mask (green) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot Prediction\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray') # Optionally show DWI in background\n",
        "        plt.imshow(pred_slice, cmap='Reds', alpha=0.3)\n",
        "        plt.title(f\"Prediction Mask (red) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def ensemble_predict(models, x, target_shape):\n",
        "        with torch.no_grad():\n",
        "            aligned_preds = []\n",
        "            for i, model in enumerate(models):\n",
        "                # Ensure model is on the correct device\n",
        "                model.to(x.device)\n",
        "                out = model(x) # Model predicts probabilities\n",
        "\n",
        "                 # Ensure out is 5D [B, C, D, H, W] for pad_or_crop_to_shape\n",
        "                while out.ndim < 5:\n",
        "                    out = out.unsqueeze(0) # Add batch or channel dim if missing\n",
        "                # Assuming smp models output [B, C, H, W] or similar, you'll need to adapt for 3D [B, C, D, H, W]\n",
        "                # If smp models are strictly 2D, this approach needs significant modification\n",
        "                # For 3D data, the output should also be 3D [B, C, D, H, W]\n",
        "\n",
        "                # Pad/crop output to match target shape\n",
        "                out_aligned = pad_or_crop_to_shape(out, target_shape)\n",
        "\n",
        "                aligned_preds.append(out_aligned)\n",
        "\n",
        "                # --- Add this print statement ---\n",
        "                print(f\"Model {i} output min/max:\", out_aligned.min().item(), out_aligned.max().item())\n",
        "                # --- End of print statement ---\n",
        "\n",
        "        # Stack the aligned predictions from each model\n",
        "        # All tensors in aligned_preds must have the same shape for stacking\n",
        "        stacked = torch.stack(aligned_preds, dim=0) # Stacks along a new dimension (number of models)\n",
        "\n",
        "        # Average the predictions across the models\n",
        "        avg = torch.mean(stacked, dim=0)\n",
        "\n",
        "        print(\"Averaged probabilities min/max:\", avg.min().item(), avg.max().item())\n",
        "\n",
        "\n",
        "        # Threshold the averaged probabilities to get the final binary mask\n",
        "        final = (avg > 0.5).float() # Use a threshold, e.g., 0.5\n",
        "\n",
        "        return final, avg # Return both the binary mask and averaged probabilities\n",
        "\n",
        "# ...existing code...\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def pad_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    # print(\"Shapes of individual tensors in ys before padding:\", [y.shape for y in ys]) # Keep for debugging if needed\n",
        "\n",
        "    # Find max height, width, depth in this batch\n",
        "    # individual tensors in xs are [C, D, H, W], ys are [D, H, W]\n",
        "    max_d = max(x.shape[1] for x in xs) # Depth\n",
        "    max_h = max(x.shape[2] for x in xs) # Height\n",
        "    max_w = max(x.shape[3] for x in xs) # Width\n",
        "\n",
        "    xs_padded = []\n",
        "    ys_padded = []\n",
        "    for x, y in zip(xs, ys):\n",
        "        # x is [C, D, H, W], y is [D, H, W]\n",
        "\n",
        "        # Padding needs to be calculated based on the current tensor's spatial dimensions\n",
        "        pad_d = max_d - x.shape[1] # Pad in Depth\n",
        "        pad_h = max_h - x.shape[2] # Pad in Height\n",
        "        pad_w = max_w - x.shape[3] # Pad in Width\n",
        "\n",
        "        # Padding order for F.pad is (W_left, W_right, H_top, H_bottom, D_front, D_back) for 5D\n",
        "        # Applied to [C, D, H, W], it pads W, H, then D. For 4D tensor x: (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
        "        x_padded = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
        "        # print(\"Shape of x_padded AFTER F.pad:\", x_padded.shape) # Debug print\n",
        "\n",
        "        # For y [D, H, W], padding order is (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
        "        y_padded = F.pad(y, (0, pad_w, 0, pad_h, 0, pad_d)) # This pads the 3D mask\n",
        "        # print(\"Shape of y_padded AFTER F.pad:\", y_padded.shape) # Debug print\n",
        "\n",
        "\n",
        "        y_padded = y_padded.unsqueeze(0) # Add channel dim at index 0 [1, D, H, W]\n",
        "        # print(\"Shape of y_padded AFTER unsqueeze:\", y_padded.shape) # Debug print\n",
        "\n",
        "\n",
        "        xs_padded.append(x_padded)\n",
        "        ys_padded.append(y_padded)\n",
        "\n",
        "    # --- Stack xs_padded here ---\n",
        "    xs_padded = torch.stack(xs_padded, dim=0) # Stacks [C, D_p, H_p, W_p] to [B, C, D_p, H_p, W_p]\n",
        "    # print(\"Shape of xs_padded AFTER stack:\", xs_padded.shape) # Debug print\n",
        "    # --- End of stacking xs_padded ---\n",
        "\n",
        "    ys_padded = torch.stack(ys_padded, dim=0) # Stacks [1, D_p, H_p, W_p] to [B, 1, D_p, H_p, W_p]\n",
        "    # print(\"Shape of ys_padded AFTER stack:\", ys_padded.shape) # Debug print\n",
        "\n",
        "    return xs_padded, ys_padded # Both are now stacked tensors\n",
        "\n",
        "\n",
        "# Helper function to get models from smp\n",
        "def get_smp_model(arch, encoder_name, in_channels, out_classes):\n",
        "    if arch == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\", # You can use pre-trained weights if available for the encoder\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5, # Adjust encoder depth if needed\n",
        "            decoder_channels=[256, 128, 64, 32, 16] # Adjust decoder channels if needed\n",
        "        )\n",
        "    elif arch == \"unetplusplus\":\n",
        "         model = smp.UnetPlusPlus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"deeplabv3plus\":\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_output_stride=16 # Common setting for DeeplabV3+\n",
        "        )\n",
        "    elif arch == \"linknet\":\n",
        "         model = smp.Linknet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture from smp: {arch}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the model architectures and their configurations for the ensemble\n",
        "# Using popular backbones from torchvision suitable for segmentation tasks\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    # \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"}, # Removed due to persistent 1x1 error\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "# %% Define ensemble_predict_slice_by_slice function\n",
        "def ensemble_predict_slice_by_slice(models, x_3d, target_shape_3d):\n",
        "    \"\"\"\n",
        "    Performs ensemble prediction on a 3D volume by processing it slice by slice.\n",
        "    Each model in the ensemble is expected to be a 2D segmentation model.\n",
        "\n",
        "    Args:\n",
        "        models (list): A list of trained 2D PyTorch segmentation models.\n",
        "        x_3d (torch.Tensor): The input 3D volume batch, shape [B, C, D, H, W].\n",
        "        target_shape_3d (tuple): The target spatial shape (D, H, W) for the output mask.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - final_ensemble_mask (torch.Tensor): The final binary 3D mask after averaging,\n",
        "              shape [B, 1, D, H, W].\n",
        "            - averaged_probs_3d (torch.Tensor): The averaged 3D probability map,\n",
        "              shape [B, 1, D, H, W].\n",
        "    \"\"\"\n",
        "    if not models:\n",
        "        print(\"No models provided for ensemble prediction.\")\n",
        "        return None, None\n",
        "\n",
        "    # Determine batch size, channels, and depth from input\n",
        "    batch_size, num_channels, depth, height, width = x_3d.shape\n",
        "    device = x_3d.device\n",
        "\n",
        "    all_models_slice_outputs = [] # To store list of 3D tensors [B, 1, D, H, W]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for model_idx, model in enumerate(models):\n",
        "            model.eval() # Ensure model is in evaluation mode\n",
        "            model_3d_output_probs = torch.zeros(\n",
        "                (batch_size, 1, target_shape_3d[0], target_shape_3d[1], target_shape_3d[2]),\n",
        "                device=device, dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            for d in range(depth): # Iterate through each slice (depth)\n",
        "                x_slice_2d = x_3d[:, :, d, :, :] # Extract 2D slice: [B, C, H, W]\n",
        "\n",
        "                # Padding for divisibility (as done in training)\n",
        "                required_divisor = 32 # Assuming this is consistent for all models\n",
        "                h_slice, w_slice = x_slice_2d.shape[2:]\n",
        "                new_h = (h_slice + required_divisor - 1) // required_divisor * required_divisor\n",
        "                new_w = (w_slice + required_divisor - 1) // required_divisor * required_divisor\n",
        "                target_padded_shape_2d = (new_h, new_w)\n",
        "\n",
        "                x_slice_padded = pad_or_crop_to_shape_2d(x_slice_2d, target_padded_shape_2d)\n",
        "\n",
        "                # Get prediction from current model for the current slice\n",
        "                model_pred_2d = model(x_slice_padded) # Output: [B, 1, H_padded, W_padded] (logits or raw output)\n",
        "                model_pred_2d = torch.sigmoid(model_pred_2d) # Apply sigmoid to get probabilities\n",
        "\n",
        "                # Align back to the original slice's spatial dimensions (H, W) or target for padding\n",
        "                # If target_shape_3d is used, the spatial shape for the slice is target_shape_3d[1:]\n",
        "                target_spatial_shape_slice = target_shape_3d[1:] # (H, W)\n",
        "                model_pred_2d_aligned = pad_or_crop_to_shape_2d(model_pred_2d, target_spatial_shape_slice)\n",
        "\n",
        "                # Ensure channel dimension is 1\n",
        "                if model_pred_2d_aligned.shape[1] != 1:\n",
        "                    model_pred_2d_aligned = model_pred_2d_aligned[:, :1, :, :] # Take the first channel\n",
        "\n",
        "                # Insert the processed 2D slice prediction into the 3D tensor\n",
        "                # Ensure dimensions match: model_3d_output_probs is [B, 1, D_target, H_target, W_target]\n",
        "                # model_pred_2d_aligned is [B, 1, H_target, W_target] (for this slice)\n",
        "                # Need to check if the current slice `d` aligns with D_target\n",
        "                if d < target_shape_3d[0]:\n",
        "                    model_3d_output_probs[:, :, d, :, :] = model_pred_2d_aligned\n",
        "\n",
        "\n",
        "            all_models_slice_outputs.append(model_3d_output_probs)\n",
        "\n",
        "    # Stack all 3D probability outputs from each model\n",
        "    # Shape will be [num_models, B, 1, D, H, W]\n",
        "    stacked_model_probs = torch.stack(all_models_slice_outputs, dim=0)\n",
        "\n",
        "    # Average the probabilities across the models (dimension 0)\n",
        "    # Resulting shape: [B, 1, D, H, W]\n",
        "    averaged_probs_3d = torch.mean(stacked_model_probs, dim=0)\n",
        "\n",
        "    # Threshold to get the final binary mask\n",
        "    final_ensemble_mask = (averaged_probs_3d > 0.5).float()\n",
        "\n",
        "    print(\"Averaged probabilities min/max:\", averaged_probs_3d.min().item(), averaged_probs_3d.max().item())\n",
        "\n",
        "    return final_ensemble_mask, averaged_probs_3d"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMPLE DATA VISUALIZATION**"
      ],
      "metadata": {
        "id": "EYp2rjGXfAQF"
      },
      "id": "EYp2rjGXfAQF"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure DEVICE is defined, e.g., 'cuda' or 'cpu'\n",
        "if 'DEVICE' not in globals():\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Ensure train_dataset is available\n",
        "if 'train_dataset' not in globals():\n",
        "    print(\"Creating train_dataset...\")\n",
        "    train_dataset = ISLESDataset3D(root_dir=DATA_DIR)\n",
        "\n",
        "# Get a sample from the dataset\n",
        "# Using the first sample (index 0) for visualization\n",
        "x_sample, y_sample = train_dataset[2]\n",
        "\n",
        "# Add batch dimension and move to device for plotting function compatibility\n",
        "x_sample = x_sample.unsqueeze(0).to(DEVICE, dtype=torch.float)\n",
        "y_sample = y_sample.unsqueeze(0).unsqueeze(0).to(DEVICE, dtype=torch.float) # Ensure [B, 1, D, H, W]\n",
        "\n",
        "# --- Find a slice with a mask for better visualization ---\n",
        "slice_index_to_plot = x_sample.shape[2] // 2 # Default to middle slice\n",
        "ground_truth_volume_for_search = y_sample[0, 0] # Get the 3D mask for the first sample [D, H, W]\n",
        "\n",
        "found_mask_slice = False\n",
        "for d in range(ground_truth_volume_for_search.shape[0]):\n",
        "    mask_slice = ground_truth_volume_for_search[d]\n",
        "    if torch.sum(mask_slice) > 0: # Check if there are any non-zero pixels in the slice\n",
        "        slice_index_to_plot = d\n",
        "        found_mask_slice = True\n",
        "        print(f\"Found slice with mask at depth index: {slice_index_to_plot}\")\n",
        "        break\n",
        "\n",
        "if not found_mask_slice:\n",
        "    print(f\"No slices with a mask found in the sample. Plotting middle slice {slice_index_to_plot} instead.\")\n",
        "\n",
        "print(f\"Visualizing sample 0 at slice index {slice_index_to_plot}\")\n",
        "\n",
        "# Call the visualization function\n",
        "# Since this is sample data without a prediction, we can pass a dummy prediction or simply omit it\n",
        "# For comprehensive visualization as per user's request (DWI, ADC, FLAIR, GT Mask)\n",
        "# we'll call plot_sample_separate_modalities. For this, we'll need a dummy prediction tensor.\n",
        "dummy_pred = torch.zeros_like(y_sample)\n",
        "\n",
        "plot_sample_separate_modalities(x_sample, y_sample, dummy_pred, slice_idx=slice_index_to_plot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "I2Yua9Twe80V",
        "outputId": "b0897292-97a3-44fd-edd3-1ba42564314d"
      },
      "id": "I2Yua9Twe80V",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train_dataset...\n",
            "entering 3D samples\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/isles-2022'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2641719262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'train_dataset'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating train_dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mISLESDataset3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Get a sample from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2410012529.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"entering 3D samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mmask_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"derivatives\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sub-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mses_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ses-0001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/isles-2022'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YX0Z9_wIcJY1",
      "metadata": {
        "id": "YX0Z9_wIcJY1"
      },
      "source": [
        "**Step 3: Data Loading and Training (Bagging Ensemble)**: This section initiates the data loading process by creating an instance of ISLESDataset3D and a DataLoader. It then sets up the training loop for multiple base models (UNet, DeepLabV3+, UNetPlusPlus, LinkNet with various ResNet encoders) as part of a 'Bagging' ensemble approach. Each model is trained independently, slice-by-slice, on the available 3D data. The notebook handles padding of 2D slices to ensure divisibility by the model's required stride and saves the state dictionary of each trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gyUbowiFbxfE",
      "metadata": {
        "id": "gyUbowiFbxfE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# %% Data Loading\n",
        "train_dataset = ISLESDataset3D(\n",
        "    root_dir=DATA_DIR)\n",
        "sample_x, sample_y = train_dataset[0]\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Set to 0 for Colab, higher on systems with multiprocessing\n",
        "    collate_fn=pad_collate\n",
        ")\n",
        "print(f\"Number of samples in dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of batches in DataLoader: {len(train_loader)}\")\n",
        "\n",
        "# %% Model Definitions using segmentation_models_pytorch\n",
        "\n",
        "\n",
        "# Define the model architectures and their configurations for the ensemble\n",
        "# Using popular backbones from torchvision suitable for segmentation tasks\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    # \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"}, # Removed due to persistent 1x1 error\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "# %% Training\n",
        "print(f\"#Train multiple base models for ensemble\")\n",
        "\n",
        "\n",
        "# ... previous imports and setup ...\n",
        "\n",
        "for model_name, config in model_architectures.items():\n",
        "    arch_name = config[\"arch\"]\n",
        "    encoder_name = config[\"encoder_name\"]\n",
        "\n",
        "    model_paths_dict[model_name] = []\n",
        "    for i in range(N_MODELS):\n",
        "        print(f\"Training {model_name} model {i+1}/{N_MODELS}, epoch 1/{EPOCHS}\")\n",
        "\n",
        "        # Get the 2D model from smp\n",
        "        try:\n",
        "             # Note: input channels is now 3 (DWI + ADC + FLAIR), output classes is 1\n",
        "             # The model expects 2D input [B, C, H, W]\n",
        "             model = get_smp_model(\n",
        "                 arch=arch_name,\n",
        "                 encoder_name=encoder_name,\n",
        "                 in_channels=3,\n",
        "                 out_classes=1\n",
        "             ).to(DEVICE)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating {model_name}: {e}\")\n",
        "            continue # Skip to the next model if creation fails\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "        criterion = dice_loss # Still using Dice Loss\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "                # x_3d is [B, C, D, H, W]\n",
        "                # y_3d is [B, 1, D, H, W]\n",
        "\n",
        "                # Process slice by slice\n",
        "                batch_losses = []\n",
        "                for d in range(x_3d.shape[2]): # Iterate through depth dimension\n",
        "                    x_slice = x_3d[:, :, d, :, :].to(DEVICE, dtype=torch.float) # Get 2D slice [B, C, H, W]\n",
        "                    y_slice = y_3d[:, :, d, :, :].to(DEVICE, dtype=torch.float) # Get 2D mask slice [B, 1, H, W]\n",
        "\n",
        "                    # --- Add padding to make height and width divisible by the required divisor and ensure minimum size ---\n",
        "                    required_divisor = 32\n",
        "                    h, w = x_slice.shape[2:]\n",
        "                    min_spatial_dim_for_bn = 128 # Increased to 128 to ensure larger feature maps and avoid 1x1 error\n",
        "\n",
        "                    new_h = max(min_spatial_dim_for_bn, (h + required_divisor - 1) // required_divisor * required_divisor)\n",
        "                    new_w = max(min_spatial_dim_for_bn, (w + required_divisor - 1) // required_divisor * required_divisor)\n",
        "                    target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                    # Use pad_or_crop_to_shape_2d to pad the input slice\n",
        "                    x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                    # Also pad the target mask slice to match the padded input slice's spatial shape\n",
        "                    y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "                    # --- End of padding ---\n",
        "\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    out_slice = model(x_slice_padded) # Use the padded slice as input\n",
        "\n",
        "                    # Now, 'out_slice' has spatial dimensions related to 'target_padded_shape_slice'\n",
        "                    # We need to align 'out_slice' with the original (or padded by pad_collate)\n",
        "                    # spatial shape of 'y_slice' for loss calculation.\n",
        "                    # The 'y_slice' already has spatial dimensions from the original padding in pad_collate.\n",
        "                    # So, align 'out_slice' to 'y_slice' spatial shape.\n",
        "\n",
        "                    target_spatial_shape_slice_for_loss = y_slice.shape[2:] # Original slice shape (H, W)\n",
        "                    # Ensure out_slice is 4D [B, C, H, W] before padding/cropping\n",
        "                    while out_slice.ndim < 4:\n",
        "                         out_slice = out_slice.unsqueeze(0)\n",
        "\n",
        "                    out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                    # Ensure both tensors have the same spatial dimensions for loss calculation\n",
        "                    # Use min of spatial dimensions for robust comparison/loss calculation\n",
        "                    min_h_loss = min(out_slice_aligned_for_loss.shape[2], y_slice_padded.shape[2])\n",
        "                    min_w_loss = min(out_slice_aligned_for_loss.shape[3], y_slice_padded.shape[3])\n",
        "\n",
        "                    out_slice_aligned_for_loss = out_slice_aligned_for_loss[:, :, :min_h_loss, :min_w_loss]\n",
        "                    y_slice_padded = y_slice_padded[:, :, :min_h_loss, :min_w_loss]\n",
        "\n",
        "\n",
        "                    loss = criterion(out_slice_aligned_for_loss, y_slice_padded) # Calculate loss using the aligned output and padded target\n",
        "                    loss.backward()\n",
        "                    batch_losses.append(loss.item())\n",
        "\n",
        "                # ... rest of the batch loop ...\n",
        "\n",
        "                # ... rest of the batch loop ...\n",
        "\n",
        "                # After processing all slices in the batch, step the optimizer\n",
        "                optimizer.step()\n",
        "                total_loss += np.mean(batch_losses) # Average loss across slices for the batch\n",
        "\n",
        "                print(f\"  {model_name} Model {i+1} Epoch {epoch+1} Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Save the trained model\n",
        "    save_path = f\"/content/drive/My Drive/{model_name}_new_model_{i}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    model_paths_dict[model_name].append(save_path)\n",
        "    print(f\"Saved {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1aa6df"
      },
      "source": [],
      "id": "ba1aa6df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7v28hA_z4J8",
      "metadata": {
        "id": "V7v28hA_z4J8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YKJHkdPv-8oL",
      "metadata": {
        "id": "YKJHkdPv-8oL"
      },
      "source": [
        "**Step 4: Load Bagging Ensemble Models and Perform Prediction**: After training, this section focuses on loading the saved state dictionaries of all the individual models trained in the Bagging phase. It then demonstrates how to perform an ensemble prediction on a new batch of data by passing the data through each loaded model slice-by-slice, applying sigmoid to get probabilities, and then averaging these probabilities across all models. The final averaged prediction mask is thresholded (e.g., at 0.5) to get a binary segmentation, and visualizations are generated for a specific slice that contains a ground truth lesion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BItLs-hxcXWa",
      "metadata": {
        "id": "BItLs-hxcXWa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tKtY5BuECBS",
      "metadata": {
        "id": "_tKtY5BuECBS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# **Step 5: Load All Trained Models and Perform Ensemble Prediction**\n",
        "# %%\n",
        "print(f\"# Loading All Trained Models from model_paths_dict for Ensemble Prediction\")\n",
        "\n",
        "# Helper function to get models from smp\n",
        "def get_smp_model(arch, encoder_name, in_channels, out_classes):\n",
        "    if arch == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"unetplusplus\":\n",
        "         model = smp.UnetPlusPlus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"deeplabv3plus\":\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_output_stride=16\n",
        "        )\n",
        "    elif arch == \"linknet\":\n",
        "         model = smp.Linknet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture from smp: {arch}\")\n",
        "\n",
        "    return model\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"},\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "# Define the base path to your Google Drive\n",
        "DRIVE_PATH = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Initialize model_paths_dict if it doesn't exist\n",
        "if 'model_paths_dict' not in globals():\n",
        "    model_paths_dict = {}\n",
        "\n",
        "# Assign the specified model paths to model_paths_dict\n",
        "model_paths_dict[\"unet_resnet18\"] = [os.path.join(DRIVE_PATH, \"unet_resnet18_new_model_0.pth\")]\n",
        "model_paths_dict[\"unetplusplus_resnet34\"] = [os.path.join(DRIVE_PATH, \"unetplusplus_resnet34_new_model_0.pth\")]\n",
        "model_paths_dict[\"deeplabv3plus_resnet50\"] = [os.path.join(DRIVE_PATH, \"deeplabv3plus_resnet50_new_model_0.pth\")]\n",
        "model_paths_dict[\"linknet_resnet18\"] = [os.path.join(DRIVE_PATH, \"linknet_resnet18_new_model_0.pth\")]\n",
        "\n",
        "# You can print the dictionary to verify\n",
        "print(\"model_paths_dict after assignment:\")\n",
        "print(model_paths_dict)\n",
        "all_loaded_models = []\n",
        "train_dataset = ISLESDataset3D(\n",
        "    root_dir=DATA_DIR)\n",
        "sample_x, sample_y = train_dataset[0]\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Set to 0 for Colab, higher on systems with multiprocessing\n",
        "    collate_fn=pad_collate\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "\n",
        "# Iterate through the model_paths_dict to load models from all categories\n",
        "# This assumes model_paths_dict is populated from previous training steps.\n",
        "if 'model_paths_dict' not in globals() or not model_paths_dict:\n",
        "     print(\"Error: 'model_paths_dict' is not populated. Please ensure models were trained and saved.\")\n",
        "else:\n",
        "    for model_name, paths in model_paths_dict.items():\n",
        "        print(f\"Loading models for: {model_name}\")\n",
        "        if model_name not in model_architectures:\n",
        "             print(f\"Warning: Configuration for '{model_name}' not found in model_architectures. Cannot recreate model structure. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        config = model_architectures[model_name]\n",
        "        arch_name = config[\"arch\"]\n",
        "        encoder_name = config[\"encoder_name\"]\n",
        "\n",
        "        for path in paths:\n",
        "            try:\n",
        "                # Recreate the model structure using the retrieved config\n",
        "                model = get_smp_model(\n",
        "                     arch=arch_name,\n",
        "                     encoder_name=encoder_name,\n",
        "                     in_channels=3, # DWI, ADC and FLAIR\n",
        "                     out_classes=1  # Binary segmentation\n",
        "                ).to(DEVICE)\n",
        "                # Load the state dictionary\n",
        "                model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "                model.eval() # Set model to evaluation mode\n",
        "                all_loaded_models.append(model)\n",
        "                print(f\"Loaded model from {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {path}: {e}\")\n",
        "                continue # Skip loading this model if there's an error\n",
        "\n",
        "print(f\"Total models loaded for ensemble prediction: {len(all_loaded_models)}\")\n",
        "\n",
        "# **Step 6: Ensemble Prediction and Visualization**\n",
        "# %%\n",
        "print(f\"# Ensemble prediction on a batch and visualization (using all loaded models)\")\n",
        "\n",
        "# Get a batch from the DataLoader for prediction\n",
        "try:\n",
        "    x_batch, y_batch = next(iter(train_loader)) # Use generic names as it's for combined ensemble\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader. Check dataset or batch size.\")\n",
        "    x_batch, y_batch = None, None\n",
        "\n",
        "if x_batch is not None and all_loaded_models: # Ensure data and models are available\n",
        "    print(\"Input batch min/max:\", x_batch.min(), x_batch.max())\n",
        "    print(\"Mask batch unique values:\", torch.unique(y_batch))\n",
        "\n",
        "    x_batch = x_batch.to(DEVICE, dtype=torch.float)\n",
        "    y_batch = y_batch.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    target_spatial_shape_3d = y_batch.shape[2:]\n",
        "\n",
        "    # Perform ensemble prediction using all loaded models\n",
        "    final_ensemble_mask, averaged_ensemble_probs = ensemble_predict_slice_by_slice(all_loaded_models, x_batch, target_spatial_shape_3d)\n",
        "\n",
        "    if final_ensemble_mask is not None:\n",
        "        sample_index_to_plot = 0 # Choose which sample in the batch to plot\n",
        "        dwi_channel = 0\n",
        "        adc_channel = 1\n",
        "        flair_channel = 2 # Added FLAIR channel\n",
        "\n",
        "        # --- Find a slice with a mask ---\n",
        "        slice_index_with_mask = None\n",
        "        # Assuming y_batch is [B, 1, D, H, W]\n",
        "        ground_truth_volume = y_batch[sample_index_to_plot, 0] # Get the 3D mask for the sample [D, H, W]\n",
        "\n",
        "        print(f\"Searching for a slice with a mask in sample {sample_index_to_plot}...\")\n",
        "        for d in range(ground_truth_volume.shape[0]): # Iterate through depth\n",
        "            mask_slice = ground_truth_volume[d]\n",
        "            if torch.sum(mask_slice) > 0: # Check if there are any non-zero pixels in the slice\n",
        "                slice_index_with_mask = d\n",
        "                print(f\"Found slice with mask at depth index: {slice_index_with_mask}\")\n",
        "                break # Stop searching once a slice with a mask is found\n",
        "\n",
        "        if slice_index_with_mask is not None:\n",
        "            slice_index_to_plot = slice_index_with_mask\n",
        "            print(f\"Plotting results for sample {sample_index_to_plot}, using slice with mask: {slice_index_to_plot}\")\n",
        "\n",
        "            # Plotting DWI input, GT, and Ensemble Prediction for the found slice\n",
        "            plot_sample_colored(\n",
        "                x_batch[sample_index_to_plot],\n",
        "                y_batch[sample_index_to_plot],\n",
        "                final_ensemble_mask[sample_index_to_plot],\n",
        "                channel=dwi_channel,\n",
        "                slice_idx=slice_index_to_plot\n",
        "            )\n",
        "\n",
        "            # Optionally, plot ADC input, GT, and Ensemble Prediction as well\n",
        "            plot_sample_colored(\n",
        "                x_batch[sample_index_to_plot],\n",
        "                y_batch[sample_index_to_plot],\n",
        "                final_ensemble_mask[sample_index_to_plot],\n",
        "                channel=adc_channel,\n",
        "                slice_idx=slice_index_to_plot\n",
        "            )\n",
        "\n",
        "            # Plot FLAIR input, GT, and Ensemble Prediction\n",
        "            plot_sample_colored(\n",
        "                x_batch[sample_index_to_plot],\n",
        "                y_batch[sample_index_to_plot],\n",
        "                final_ensemble_mask[sample_index_to_plot],\n",
        "                channel=flair_channel,\n",
        "                slice_idx=slice_index_to_plot\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(f\"No slices with a mask found in sample {sample_index_to_plot}. Cannot plot a slice with a visible ground truth mask.\")\n",
        "            # Optionally, plot a default slice anyway, but inform the user it has no mask\n",
        "            # slice_index_to_plot = final_ensemble_mask.shape[2] // 2 # Default to middle slice\n",
        "            # print(f\"No mask found in sample {sample_index_to_plot}. Plotting middle slice {slice_index_to_plot} anyway.\")\n",
        "            # plot_sample_colored(...) # Call plotting with the default slice\n",
        "        # --- End of finding slice with mask ---\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Ensemble prediction failed.\")\n",
        "\n",
        "else:\n",
        "    if x_batch is None:\n",
        "         print(\"Skipping ensemble prediction as no data batch was loaded.\")\n",
        "    if not all_loaded_models:\n",
        "         print(\"Skipping ensemble prediction as no models were loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Boosting Ensemble Training and Prediction:** This section introduces the concept of a 'Boosting' ensemble. It sets up a sequential training process for multiple models (using a unet_resnet18 configuration). In a typical boosting scenario, subsequent models focus on correcting errors made by previous models. While the notebook's implementation for weight calculation is a simplified placeholder, it trains models sequentially and saves them. Subsequently, it loads these boosting models and performs an ensemble prediction on a batch of data, calculating and displaying metrics like Dice score and visualizing the result."
      ],
      "metadata": {
        "id": "7s8eyvY8B1rk"
      },
      "id": "7s8eyvY8B1rk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BGM3vm4p68Jd",
      "metadata": {
        "id": "BGM3vm4p68Jd"
      },
      "outputs": [],
      "source": [
        "# %% Boosting Ensemble Training (Simplified Sequential)\n",
        "print(f\"#Training Boosting Ensemble Base Models (Simplified Sequential)\")\n",
        "\n",
        "boosting_models = []\n",
        "N_BOOSTING_MODELS = 3 # Number of models in the boosting sequence\n",
        "BOOSTING_MODEL_CONFIG = \"unet_resnet18_config1\" # Use one specific configuration for boosting\n",
        "\n",
        "# You will need a way to calculate sample/pixel weights based on previous model errors.\n",
        "# This is a conceptual placeholder.\n",
        "def calculate_error_weights(ground_truth, previous_model_predictions):\n",
        "    # Implement logic to calculate weights based on where the previous model\n",
        "    # made errors or was uncertain.\n",
        "    # Example: weights could be higher for misclassified pixels.\n",
        "    # This would require running inference on the training data after each boosting step.\n",
        "    return torch.ones_like(ground_truth).to(ground_truth.device) # Placeholder: uniform weights\n",
        "\n",
        "\n",
        "# Define the model configurations for the boosting ensemble\n",
        "model_configurations = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"},\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "# Choose which configuration to use for the boosting models\n",
        "BOOSTING_MODEL_CONFIG = \"unet_resnet18\" # Example: Use the UNet with ResNet18 encoder\n",
        "\n",
        "# --- Your boosting training loop starts here ---\n",
        "# ...\n",
        "for i in range(N_BOOSTING_MODELS):\n",
        "    print(f\"  Training Boosting model {i+1}/{N_BOOSTING_MODELS}\")\n",
        "\n",
        "    # Get model (using smp helper or custom 3D model) - use the same architecture\n",
        "    config = model_configurations[BOOSTING_MODEL_CONFIG]\n",
        "    # ... rest of your boosting code ...\n",
        "\n",
        "\n",
        "for i in range(N_BOOSTING_MODELS):\n",
        "    print(f\"  Training Boosting model {i+1}/{N_BOOSTING_MODELS}\")\n",
        "\n",
        "    # Get model (using smp helper or custom 3D model) - use the same architecture\n",
        "    config = model_configurations[BOOSTING_MODEL_CONFIG]\n",
        "    try:\n",
        "         if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "             model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=3, out_classes=1).to(DEVICE)\n",
        "         # elif config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "         #      model = UNet3D(...).to(DEVICE) # Initialize your 3D model\n",
        "         else:\n",
        "              raise ValueError(f\"Unknown architecture: {config['arch']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating boosting model {i+1}: {e}\")\n",
        "        continue # Skip to the next model instance\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = dice_loss # You might use a weighted loss\n",
        "\n",
        "    # --- Training Loop (Slice-by-Slice if using smp) ---\n",
        "    # This will be similar to the bagging training loop, but you'll need\n",
        "    # to incorporate error weights if you implement weighted training.\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "             x_3d = x_3d.to(DEVICE, dtype=torch.float)\n",
        "             y_3d = y_3d.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "             # Calculate error weights based on the previous model's predictions (if i > 0)\n",
        "             # This part is complex and requires running inference on the training data\n",
        "             # with the previously trained model.\n",
        "             # For this simplified example, we'll use uniform weights for now.\n",
        "             error_weights = calculate_error_weights(y_3d, None) # Placeholder\n",
        "\n",
        "\n",
        "             # --- Slice-by-Slice Processing (if using smp) ---\n",
        "             batch_losses = []\n",
        "             for d in range(x_3d.shape[2]):\n",
        "                 x_slice = x_3d[:, :, d, :, :].clone() # Get slice\n",
        "                 y_slice = y_3d[:, :, d, :, :].clone() # Get slice\n",
        "                 weights_slice = error_weights[:, :, d, :, :].clone() # Get corresponding weights slice\n",
        "\n",
        "\n",
        "                 # Padding for divisibility\n",
        "                 required_divisor = 32\n",
        "                 h, w = x_slice.shape[2:]\n",
        "                 new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                 x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                 y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "                 weights_slice_padded = pad_or_crop_to_shape_2d(weights_slice, target_padded_shape_slice)\n",
        "\n",
        "\n",
        "                 optimizer.zero_grad()\n",
        "                 out_slice = model(x_slice_padded)\n",
        "\n",
        "                 # Align output for loss\n",
        "                 target_spatial_shape_slice_for_loss = y_slice.shape[2:]\n",
        "                 out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                 # Use a weighted loss function\n",
        "                 # You would need to implement a weighted Dice Loss or similar\n",
        "                 # For this example, using regular Dice Loss\n",
        "                 loss = criterion(out_slice_aligned_for_loss, y_slice) # Use y_slice for loss\n",
        "                 # loss = weighted_dice_loss(out_slice_aligned_for_loss, y_slice_padded, weights_slice_padded) # If using weighted loss\n",
        "\n",
        "\n",
        "                 loss.backward()\n",
        "                 batch_losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += np.mean(batch_losses)\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"    Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "    print(f\"  Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # After training, save the boosting model instance\n",
        "    save_path = f\"boosting_{BOOSTING_MODEL_CONFIG.replace(' ', '_')}_instance_{i}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    boosting_models.append({\"path\": save_path, \"config_name\": BOOSTING_MODEL_CONFIG}) # Store path and config name\n",
        "    print(f\"  Saved {save_path}\")\n",
        "\n",
        "    # Important for boosting: After training a model, you need to run inference\n",
        "    # on the *entire* training dataset with this model to calculate the error weights\n",
        "    # for the *next* boosting model. This requires another inference loop over the\n",
        "    # training data, which can be time-consuming.\n",
        "    # For this conceptual example, we are skipping the actual error weight calculation.\n",
        "\n",
        "\n",
        "# %% Boosting Ensemble Prediction\n",
        "print(f\"#Boosting Ensemble Prediction\")\n",
        "\n",
        "# Load boosting models from paths\n",
        "loaded_boosting_models = []\n",
        "for model_info in boosting_models:\n",
        "     path = model_info[\"path\"]\n",
        "     config_name = model_info[\"config_name\"]\n",
        "\n",
        "     try:\n",
        "         # Find the original configuration from model_configurations\n",
        "         original_config = model_configurations.get(config_name)\n",
        "\n",
        "         if original_config:\n",
        "              if original_config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "                 model = get_smp_model(arch=original_config[\"arch\"], encoder_name=original_config[\"encoder_name\"], in_channels=3, out_classes=1).to(DEVICE)\n",
        "             # elif original_config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "             #      model = UNet3D(...).to(DEVICE)\n",
        "              else:\n",
        "                   print(f\"Unknown architecture in saved path: {original_config['arch']}\")\n",
        "                   continue\n",
        "\n",
        "              model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "              model.eval()\n",
        "              loaded_boosting_models.append(model)\n",
        "              print(f\"  Loaded boosting model from {path}\")\n",
        "         else:\n",
        "             print(f\"  Could not find configuration for saved model path: {path}\")\n",
        "             continue\n",
        "\n",
        "     except Exception as e:\n",
        "         print(f\"  Error loading boosting model from {path}: {e}\")\n",
        "         continue\n",
        "\n",
        "print(f\"Number of loaded boosting models: {len(loaded_boosting_models)}\")\n",
        "\n",
        "# Get a batch from the DataLoader for prediction\n",
        "# Using train_loader here for demonstration, replace with a validation/test loader\n",
        "try:\n",
        "    x_batch_bagging, y_batch_bagging = next(iter(train_loader))\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader. Check dataset or batch size.\")\n",
        "    x_batch_bagging, y_batch_bagging = None, None # Handle empty loader\n",
        "\n",
        "if x_batch_bagging is not None:\n",
        "    print(\"Input batch min/max:\", x_batch_bagging.min(), x_batch_bagging.max())\n",
        "    print(\"Mask batch unique values:\", torch.unique(y_batch_bagging))\n",
        "\n",
        "    # Ensure correct dims and type for prediction and move to device\n",
        "    x_batch_bagging = x_batch_bagging.to(DEVICE, dtype=torch.float)\n",
        "    y_batch_bagging = y_batch_bagging.to(DEVICE, dtype=torch.float) # Also move target mask to device\n",
        "\n",
        "    # Define the target shape for alignment - this should match the shape expected by your padding/cropping functions\n",
        "    # You might need to determine this based on your data and pad_collate output\n",
        "    target_spatial_shape_3d_bagging = y_batch_bagging.shape[2:] # Example: Assuming y_batch_bagging is [B, 1, D, H, W]\n",
        "\n",
        "    # --- Your boosting ensemble prediction code starts here ---\n",
        "    print(f\"# Boosting Ensemble prediction on a batch\")\n",
        "\n",
        "    # Use ensemble_predict_slice_by_slice (or a similar function) for prediction\n",
        "    # This will average the predictions of the loaded boosting models.\n",
        "    final_boosting_mask, averaged_boosting_probs = ensemble_predict_slice_by_slice(loaded_boosting_models, x_batch_bagging, target_spatial_shape_3d_bagging)\n",
        "    # ... rest of your boosting evaluation code ...\n",
        "else:\n",
        "    print(\"Skipping boosting ensemble prediction as no data batch was loaded.\")\n",
        "\n",
        "if len(loaded_boosting_models) > 0:\n",
        "    # Use ensemble_predict_slice_by_slice (or a similar function) for prediction\n",
        "    # This will average the predictions of the loaded boosting models.\n",
        "    final_boosting_mask, averaged_boosting_probs = ensemble_predict_slice_by_slice(loaded_boosting_models, x_batch_bagging, target_spatial_shape_3d_bagging) # Reuse batch from bagging\n",
        "\n",
        "    # Evaluate Boosting Ensemble prediction\n",
        "    boosting_dice = dice_score(final_boosting_mask, safe_unsqueeze_mask(y_batch_bagging.to(DEVICE)))\n",
        "    print(f\"Boosting Ensemble Dice Score: {boosting_dice:.4f}\")\n",
        "\n",
        "    # Plot Boosting Ensemble prediction for a sample\n",
        "    plot_sample_colored(x_batch_bagging[0], y_batch_bagging[0], final_boosting_mask[0], channel=0)\n",
        "    plot_sample_separate_modalities(x_batch_bagging[0], y_batch_bagging[0], final_boosting_mask[0])\n",
        "\n",
        "else:\n",
        "     print(\"No boosting models were loaded for prediction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BXSpvJgdAExs",
      "metadata": {
        "id": "BXSpvJgdAExs"
      },
      "source": [
        "**Step 6: Stacking Ensemble Training and Prediction**: This part of the notebook focuses on the 'Stacking' ensemble. It trains several base models (UNet, DeepLabV3+, UNetPlusPlus, LinkNet) individually, similar to the initial bagging phase, and saves them. The primary goal of stacking is to use the predictions of these base models as 'meta-features' for a meta-learner (which would then learn to combine them). In the current notebook, the prediction phase for stacking simplifies this by directly averaging the outputs of these base models, similar to bagging, and evaluates its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5Fx5UYQ_0GS",
      "metadata": {
        "id": "w5Fx5UYQ_0GS"
      },
      "outputs": [],
      "source": [
        "# %% Stacking Ensemble Training - Base Models\n",
        "print(f\"#Training Stacking Ensemble Base Models\")\n",
        "\n",
        "import pandas as pd\n",
        "stacking_base_models = {}\n",
        "N_STACKING_BASE_MODELS = 1 # Train one instance of each configuration for stacking\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    # \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"}, # Removed due to persistent 1x1 error\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "for model_name, config in model_architectures .items():\n",
        "    stacking_base_models[model_name] = []\n",
        "    print(f\"  Training Stacking base model {model_name}\")\n",
        "\n",
        "    # Get model\n",
        "    try:\n",
        "         if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "             model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=3, out_classes=1).to(DEVICE)\n",
        "         # elif config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "         #      model = UNet3D(...).to(DEVICE)\n",
        "         else:\n",
        "              raise ValueError(f\"Unknown architecture: {config['arch']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating stacking base model {model_name}: {e}\")\n",
        "        continue # Skip to the next model\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = dice_loss\n",
        "\n",
        "    # --- Training Loop (Similar to Bagging/Boosting) ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "             x_3d = x_3d.to(DEVICE, dtype=torch.float)\n",
        "             y_3d = y_3d.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "             # --- Slice-by-Slice Processing (if using smp) ---\n",
        "             batch_losses = []\n",
        "             for d in range(x_3d.shape[2]):\n",
        "                 x_slice = x_3d[:, :, d, :, :].clone()\n",
        "                 y_slice = y_3d[:, :, d, :, :].clone()\n",
        "\n",
        "                 # Padding for divisibility\n",
        "                 required_divisor = 32\n",
        "                 h, w = x_slice.shape[2:]\n",
        "                 new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                 x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                 y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "\n",
        "\n",
        "                 optimizer.zero_grad()\n",
        "                 out_slice = model(x_slice_padded)\n",
        "\n",
        "                 # Align output for loss\n",
        "                 target_spatial_shape_slice_for_loss = y_slice.shape[2:]\n",
        "                 out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                 loss = criterion(out_slice_aligned_for_loss, y_slice)\n",
        "\n",
        "                 loss.backward()\n",
        "                 batch_losses.append(loss.item())\n",
        "\n",
        "             # --- End of Slice-by-Slice Processing ---\n",
        "\n",
        "             optimizer.step()\n",
        "             total_loss += np.mean(batch_losses)\n",
        "\n",
        "             if batch_idx % 10 == 0:\n",
        "                print(f\"    Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "        print(f\"  Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Save the trained base model\n",
        "    save_path = f\"stacking_base_{model_name.replace(' ', '_')}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    stacking_base_models[model_name].append(save_path)\n",
        "    print(f\"  Saved {save_path}\")\n",
        "\n",
        "\n",
        "# %% Stacking Ensemble - Generate Meta-Features on Validation Data\n",
        "print(f\"#Generating Meta-Features for Stacking\")\n",
        "\n",
        "# Load trained base models\n",
        "loaded_stacking_base_models = {}\n",
        "for model_name, paths in stacking_base_models.items():\n",
        "    if paths: # Assuming only one instance per config for stacking base\n",
        "        path = paths[0]\n",
        "        try:\n",
        "            config = model_architectures[model_name]\n",
        "            if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]:\n",
        "                 model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=3, out_classes=1).to(DEVICE)\n",
        "            # elif config[\"arch\"] == \"unet3d\": model = UNet3D(...).to(DEVICE)\n",
        "            else:\n",
        "                print(f\"Unknown architecture for stacking base model: {config['arch']}\")\n",
        "                continue\n",
        "\n",
        "            model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "            model.eval()\n",
        "            loaded_stacking_base_models[model_name] = model\n",
        "            print(f\"  Loaded stacking base model from {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading stacking base model from {path}: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"  No saved model found for stacking base config: {model_name}\")\n",
        "\n",
        "\n",
        "        # %% Stacking Ensemble Prediction\n",
        "# Predict data using the trained stacking base models (average their outputs)\n",
        "\n",
        "print(\"# Stacking Ensemble Prediction\")\n",
        "\n",
        "all_results = {}\n",
        "# Use stacking base models as the ensemble (list of models)\n",
        "stacking_models_list = list(loaded_stacking_base_models.values()) if loaded_stacking_base_models else []\n",
        "\n",
        "if len(stacking_models_list) > 0:\n",
        "    # Get a batch from the DataLoader\n",
        "    try:\n",
        "        x_batch_stacking, y_batch_stacking = next(iter(train_loader))\n",
        "    except StopIteration:\n",
        "        x_batch_stacking, y_batch_stacking = None, None\n",
        "\n",
        "    if x_batch_stacking is not None:\n",
        "        x_batch_stacking = x_batch_stacking.to(DEVICE, dtype=torch.float)\n",
        "        y_batch_stacking = y_batch_stacking.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "        # Target shape for alignment (D, H, W)\n",
        "        target_spatial_shape_stacking = x_batch_stacking.shape[2:]\n",
        "\n",
        "        # Run stacking ensemble prediction (same as bagging: average base model outputs)\n",
        "        final_stacking_mask, averaged_stacking_probs = ensemble_predict_slice_by_slice(\n",
        "            stacking_models_list, x_batch_stacking, target_spatial_shape_stacking\n",
        "        )\n",
        "\n",
        "        if final_stacking_mask is not None:\n",
        "            print(\"Stacking ensemble prediction complete.\")\n",
        "\n",
        "            # Optional: Dice score on first sample\n",
        "            sample_idx = 0\n",
        "            y_s = y_batch_stacking[sample_idx:sample_idx+1]\n",
        "            p_s = final_stacking_mask[sample_idx:sample_idx+1]\n",
        "            if y_s.numel() > 0 and p_s.numel() > 0:\n",
        "                d = dice_score(p_s, safe_unsqueeze_mask(y_s))\n",
        "                print(f\"Sample {sample_idx} Dice (stacking): {d.item():.4f}\")\n",
        "\n",
        "            # Plot a sample slice (e.g. middle slice)\n",
        "            slice_idx_plot = x_batch_stacking.shape[2] // 2\n",
        "            x_slice_plot = x_batch_stacking[0:1, :, slice_idx_plot, :, :]\n",
        "            y_slice_plot = y_batch_stacking[0:1, :, slice_idx_plot, :, :]\n",
        "            pred_slice_plot = final_stacking_mask[0:1, :, slice_idx_plot, :, :]\n",
        "            if pred_slice_plot.shape[1] != 1:\n",
        "                pred_slice_plot = pred_slice_plot[:, :1, ...]\n",
        "            plot_sample_colored(x_slice_plot, y_slice_plot, pred_slice_plot, slice_idx=0)\n",
        "        else:\n",
        "            print(\"Stacking ensemble prediction returned None.\")\n",
        "    else:\n",
        "        print(\"No batch available for stacking prediction.\")\n",
        "else:\n",
        "    print(\"No stacking base models loaded. Run the stacking training cell first.\")\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "# Assuming final_stacking_mask and y_batch_stacking are available from previous steps\n",
        "\n",
        "if 'final_stacking_mask' in globals() and 'y_batch_stacking' in globals():\n",
        "    # Calculate metrics for the Stacking ensemble\n",
        "    stacking_ensemble_dice = dice_score(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking)).item()\n",
        "    stacking_ensemble_avd = absolute_volume_difference(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking))\n",
        "    stacking_ensemble_f1 = lesion_wise_f1_score(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking))\n",
        "\n",
        "    stacking_results_df = pd.DataFrame({\n",
        "        'model_name': ['Stacking Ensemble'],\n",
        "        'avg_dice': [stacking_ensemble_dice],\n",
        "        'avg_avd': [stacking_ensemble_avd],\n",
        "        'avg_f1': [stacking_ensemble_f1]\n",
        "    })\n",
        "\n",
        "    print(\"\\nStacking Ensemble Performance:\")\n",
        "    display(stacking_results_df)\n",
        "\n",
        "    # Store metrics for overall summary later\n",
        "    all_results[\"Stacking Ensemble\"] = stacking_results_df.set_index('model_name').to_dict('index')\n",
        "else:\n",
        "    print(\"Stacking ensemble prediction or ground truth not found. Please ensure previous steps ran successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774d3081",
      "metadata": {
        "id": "774d3081"
      },
      "source": [
        "**Step 7: Evaluate Individual Ensemble Models**: This step provides a detailed evaluation of each individual model that was part of the initial Bagging ensemble. For each model, it iterates through slices of a test batch, makes predictions, and calculates key metrics such as Dice Score, Absolute Volume Difference (AVD), and F1 Score. It then aggregates these metrics to provide an average performance for each individual model and also plots a sample prediction from each model for visual comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vPSRC0eT_svq",
      "metadata": {
        "id": "vPSRC0eT_svq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming model_paths_dict is already populated and load_ensemble is defined\n",
        "\n",
        "# Load the ensemble models\n",
        "ensemble_models = load_ensemble(model_paths_dict, DEVICE)\n",
        "\n",
        "print(f\"# Evaluating individual ensemble models and plotting samples\")\n",
        "\n",
        "# Get one batch from the DataLoader for evaluation\n",
        "try:\n",
        "    x_batch_eval, y_batch_eval = next(iter(train_loader))\n",
        "except StopIteration:\n",
        "    print(\"Could not get a batch from the DataLoader. Make sure the DataLoader is not empty.\")\n",
        "    x_batch_eval = None\n",
        "    y_batch_eval = None\n",
        "\n",
        "# Initialize a list to store metrics for each model\n",
        "model_metrics_list = []\n",
        "\n",
        "# Choose a slice index to plot from the evaluation batch\n",
        "# You might want to choose a slice that contains the lesion if possible\n",
        "slice_idx_to_plot_individual = x_batch_eval.shape[2] // 2\n",
        "print(f\"Plotting slice index {slice_idx_to_plot_individual} for individual model evaluation.\")\n",
        "\n",
        "# Check if a batch was successfully loaded and if there are models to evaluate\n",
        "if x_batch_eval is not None and len(ensemble_models) > 0:\n",
        "    x_batch_eval = x_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "    y_batch_eval = y_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    # Ensure y_batch_eval has a channel dimension [B, 1, D, H, W]\n",
        "    if y_batch_eval.ndim == 4:\n",
        "        y_batch_eval = y_batch_eval.unsqueeze(1)\n",
        "\n",
        "    for i, model in enumerate(ensemble_models):\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        print(f\"  Evaluating individual model {i}...\")\n",
        "        with torch.no_grad():\n",
        "            slice_metrics = []\n",
        "            predicted_slice_for_plotting = None # To store the prediction for the chosen slice\n",
        "\n",
        "            for d in range(x_batch_eval.shape[2]): # Iterate through depth dimension\n",
        "                x_slice = x_batch_eval[:, :, d, :, :]\n",
        "                y_slice = y_batch_eval[:, :, d, :, :]\n",
        "\n",
        "                required_divisor = 32 # Example divisor\n",
        "                h, w = x_slice.shape[2:]\n",
        "                new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                target_padded_shape_slice = (new_h, new_w)\n",
        "                x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "\n",
        "                out_slice = model(x_slice_padded)\n",
        "                out_slice = torch.sigmoid(out_slice) # Apply sigmoid for probabilities\n",
        "\n",
        "                target_spatial_shape_slice_for_metrics = y_slice.shape[2:]\n",
        "                out_slice_aligned = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_metrics)\n",
        "\n",
        "                pred_slice = (out_slice_aligned > 0.5).float()\n",
        "\n",
        "                # Calculate metrics for the slice\n",
        "                try:\n",
        "                    dice = dice_score(pred_slice, y_slice)\n",
        "                    avd = absolute_volume_difference(pred_slice, y_slice)\n",
        "                    f1 = lesion_wise_f1_score(pred_slice, y_slice)\n",
        "                    slice_metrics.append({\"dice\": dice, \"avd\": avd, \"f1\": f1})\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error calculating metrics for slice {d} of model {i}: {e}\")\n",
        "                    continue # Skip metrics for this slice if calculation fails\n",
        "\n",
        "\n",
        "                # Store the prediction for the chosen plotting slice\n",
        "                if d == slice_idx_to_plot_individual:\n",
        "                     predicted_slice_for_plotting = pred_slice\n",
        "\n",
        "\n",
        "            # After processing all slices, calculate average metrics for the volume\n",
        "            if slice_metrics:\n",
        "                 avg_dice = np.mean([m['dice'].cpu() for m in slice_metrics])\n",
        "                 avg_avd = np.mean([m['avd'] for m in slice_metrics])\n",
        "                 avg_f1 = np.mean([m['f1'] for m in slice_metrics])\n",
        "\n",
        "                 model_metrics_list.append({\n",
        "                    \"model_index\": i,\n",
        "                    \"avg_dice\": avg_dice,\n",
        "                    \"avg_avd\": avg_avd,\n",
        "                    \"avg_f1\": avg_f1\n",
        "                 })\n",
        "                 print(f\"  Model {i} (Individual): Avg Dice: {avg_dice:.4f}, Avg AVD: {avg_avd:.4f}, Avg F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Plot the chosen slice for this individual model\n",
        "                 if predicted_slice_for_plotting is not None:\n",
        "                     print(f\"  Plotting prediction for model {i}, slice {slice_idx_to_plot_individual}\")\n",
        "                     # Pass the input batch, ground truth batch, and the single predicted slice\n",
        "                     # You might need to adjust plot_sample_colored to handle a single slice prediction\n",
        "                     # Let's pass the relevant slices for plotting\n",
        "                     x_slice_plot_4d = x_batch_eval[0:1, :, slice_idx_to_plot_individual, :, :]\n",
        "                     y_slice_plot_4d = y_batch_eval[0:1, :, slice_idx_to_plot_individual, :, :]\n",
        "\n",
        "                     # Corrected: predicted_slice_for_plotting is 4D [B, C, H, W]\n",
        "                     predicted_slice_for_plotting_4d = predicted_slice_for_plotting[0:1, :, :, :] # Access all dimensions\n",
        "\n",
        "                     # Ensure predicted_slice_for_plotting_4d is [B, 1, H, W]\n",
        "                     if predicted_slice_for_plotting_4d.shape[1] != 1:\n",
        "                         predicted_slice_for_plotting_4d = predicted_slice_for_plotting_4d[:, :1, ...] # Take the first channel\n",
        "\n",
        "                     # Ensure y_slice_plot_4d is [B, 1, H, W]\n",
        "                     if y_slice_plot_4d.shape[1] != 1:\n",
        "                         y_slice_plot_4d = y_slice_plot_4d[:, :1, ...]\n",
        "\n",
        "\n",
        "                     plot_sample_colored(x_slice_plot_4d, y_slice_plot_4d, predicted_slice_for_plotting_4d, slice_idx=0) # slice_idx=0 because we're passing a single slice as a batch\n",
        "                 else:\n",
        "                     print(f\"  Could not plot sample for model {i}, slice {slice_idx_to_plot_individual}.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                 print(f\"  No slice metrics calculated for model {i}. Check for errors during slice processing.\")\n",
        "\n",
        "\n",
        "    # Print summary of individual model metrics\n",
        "    print(\"\\nIndividual Model Evaluation Summary:\")\n",
        "    for metrics in model_metrics_list:\n",
        "        print(f\"  Model {metrics['model_index']}: Avg Dice={metrics['avg_dice']:.4f}, Avg AVD={metrics['avg_avd']:.4f}, Avg F1={metrics['avg_f1']:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping individual model evaluation: No batch loaded or no models loaded.\")\n",
        "\n",
        "\n",
        "# **Step 6: Perform Ensemble Prediction and Plot**\n",
        "print(f\"\\n# Performing Ensemble Prediction and Plotting\")\n",
        "\n",
        "# Assuming you have loaded the ensemble models into `ensemble_models` previously\n",
        "# And you have a batch of data for evaluation (e.g., x_batch_eval, y_batch_eval)\n",
        "\n",
        "if x_batch_eval is not None and len(ensemble_models) > 0:\n",
        "    # Perform ensemble prediction on the evaluation batch\n",
        "    # The ensemble_predict_slice_by_slice function already handles the slice-by-slice\n",
        "    # processing and averaging.\n",
        "    ensemble_pred_mask, ensemble_avg_probs = ensemble_predict_slice_by_slice(\n",
        "        ensemble_models, x_batch_eval, target_shape_3d=x_batch_eval.shape[2:]\n",
        "        ) # Use the original batch depth as target shape\n",
        "\n",
        "    if ensemble_pred_mask is not None:\n",
        "        print(\"Ensemble prediction complete.\")\n",
        "\n",
        "        # Choose a slice index to plot for the ensemble result\n",
        "        slice_idx_to_plot_ensemble = x_batch_eval.shape[2] // 2\n",
        "        print(f\"Plotting slice index {slice_idx_to_plot_ensemble} for ensemble prediction.\")\n",
        "\n",
        "        # Plot the input, ground truth, and ensemble prediction for the chosen slice\n",
        "        # Extract the chosen slice\n",
        "        x_slice_plot_ensemble = x_batch_eval[0:1, :, slice_idx_to_plot_ensemble, :, :]\n",
        "        y_slice_plot_ensemble = y_batch_eval[0:1, :, slice_idx_to_plot_ensemble, :, :]\n",
        "        pred_slice_plot_ensemble = ensemble_pred_mask[0:1, :, slice_idx_to_plot_ensemble, :, :]\n",
        "\n",
        "        # Ensure pred_slice_plot_ensemble is [B, 1, H, W]\n",
        "        if pred_slice_plot_ensemble.shape[1] != 1:\n",
        "            pred_slice_plot_ensemble = pred_slice_plot_ensemble[:, :1, ...]\n",
        "\n",
        "        # Ensure y_slice_plot_ensemble is [B, 1, H, W]\n",
        "        if y_slice_plot_ensemble.shape[1] != 1:\n",
        "            y_slice_plot_ensemble = y_slice_plot_ensemble[:, :1, ...]\n",
        "\n",
        "\n",
        "        plot_sample_colored(x_slice_plot_ensemble, y_slice_plot_ensemble, pred_slice_plot_ensemble, slice_idx=0) # slice_idx=0 because we're passing a single slice as a batch\n",
        "\n",
        "    else:\n",
        "        print(\"Ensemble prediction failed. Cannot plot.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping ensemble prediction and plotting: No batch loaded or no models loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e93ab749"
      },
      "source": [
        "## 5.1 Introduction to Results and Discussion\n",
        "\n",
        "### Subtask:\n",
        "Provide a brief introduction to the chapter, outlining its purpose and reiterating the objectives of the study.\n"
      ],
      "id": "e93ab749"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb5598fb"
      },
      "source": [
        "### 5.1 Introduction to Results and Discussion\n",
        "\n",
        "This chapter is dedicated to the comprehensive presentation, rigorous analysis, and insightful interpretation of the experimental results obtained from the various medical image segmentation models and ensemble methods employed in this study. Our primary objective is to evaluate the performance of individual deep learning segmentation models, critically compare different ensemble strategies including Bagging, Boosting, and Stacking, and ultimately identify the most effective and robust approach for accurate brain lesion segmentation in multi-modal MRI data. Through this detailed examination, we aim to provide a clear understanding of the strengths and weaknesses of each method and their combined efficacy in addressing the challenges of medical image analysis."
      ],
      "id": "eb5598fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a90a72d"
      },
      "source": [
        "## 5.2 Experimental Results\n",
        "\n",
        "### Subtask:\n",
        "Detail the quantitative outcomes of all experiments, including tables and figures summarizing the performance metrics obtained from individual base models and ensemble approaches.\n"
      ],
      "id": "9a90a72d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e05e96ea"
      },
      "source": [
        "## 5.2 Experimental Results\n",
        "\n",
        "This section presents the quantitative outcomes of the segmentation experiments. It summarizes the performance metrics obtained from individual base models (trained for the bagging ensemble) and the different ensemble approaches (bagging, boosting, and stacking) using tables and figures. We will evaluate each approach based on metrics such as Dice Score, Absolute Volume Difference (AVD), and F1 Score."
      ],
      "id": "e05e96ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97a30823"
      },
      "source": [
        "**Reasoning**:\n",
        "To clearly present the individual model performance, I will create a pandas DataFrame from the collected metrics and display it.\n",
        "\n"
      ],
      "id": "97a30823"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03cc661a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize all_results globally to ensure it's always defined\n",
        "all_results = {}\n",
        "\n",
        "# Assuming model_metrics_list contains the metrics from the previous cell's execution\n",
        "# If model_metrics_list is empty from a fresh run, re-run the previous cell.\n",
        "\n",
        "if model_metrics_list:\n",
        "    individual_model_results_df = pd.DataFrame(model_metrics_list)\n",
        "    # Map model_index to actual model names for better readability\n",
        "    model_names_map = {\n",
        "        0: \"unet_resnet18\",\n",
        "        1: \"deeplabv3plus_resnet50\",\n",
        "        2: \"unetplusplus_resnet34\",\n",
        "        3: \"linknet_resnet18\"\n",
        "    }\n",
        "    individual_model_results_df['model_name'] = individual_model_results_df['model_index'].map(model_names_map)\n",
        "    individual_model_results_df = individual_model_results_df[['model_name', 'avg_dice', 'avg_avd', 'avg_f1']]\n",
        "\n",
        "    print(\"\\nIndividual Model Performance:\")\n",
        "    display(individual_model_results_df)\n",
        "\n",
        "    # Store metrics for overall summary later\n",
        "    all_results[\"Individual Models\"] = individual_model_results_df.set_index('model_name').to_dict('index')\n",
        "else:\n",
        "    print(\"No individual model metrics found to display. Please ensure the previous evaluation cell ran successfully.\")"
      ],
      "id": "03cc661a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d354e0"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires detailing experimental results. I need to display the quantitative outcomes of the individual base models and store them for later summarization. The previous code block already handles the generation and display of the DataFrame.\n",
        "\n"
      ],
      "id": "74d354e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7abf8bd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming model_metrics_list contains the metrics from the previous cell's execution\n",
        "# If model_metrics_list is empty from a fresh run, re-run the previous cell.\n",
        "\n",
        "if model_metrics_list:\n",
        "    individual_model_results_df = pd.DataFrame(model_metrics_list)\n",
        "    # Map model_index to actual model names for better readability\n",
        "    model_names_map = {\n",
        "        0: \"unet_resnet18\",\n",
        "        1: \"deeplabv3plus_resnet50\",\n",
        "        2: \"unetplusplus_resnet34\",\n",
        "        3: \"linknet_resnet18\"\n",
        "    }\n",
        "    individual_model_results_df['model_name'] = individual_model_results_df['model_index'].map(model_names_map)\n",
        "    individual_model_results_df = individual_model_results_df[['model_name', 'avg_dice', 'avg_avd', 'avg_f1']]\n",
        "\n",
        "    print(\"\\nIndividual Model Performance:\")\n",
        "    display(individual_model_results_df)\n",
        "else:\n",
        "    print(\"No individual model metrics found to display. Please ensure the previous evaluation cell ran successfully.\")\n",
        "\n",
        "# Store metrics for overall summary later\n",
        "all_results = {\"Individual Models\": individual_model_results_df.set_index('model_name').to_dict('index')} if 'individual_model_results_df' in locals() else {}"
      ],
      "id": "f7abf8bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5145476"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires detailing experimental results. I need to calculate and display the performance metrics for the Bagging ensemble (which corresponds to the main ensemble prediction in the notebook) and store these results for later summarization. I will calculate Dice, AVD, and F1 for the Bagging ensemble prediction that was already generated.\n",
        "\n"
      ],
      "id": "c5145476"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92ab9c3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming ensemble_pred_mask and y_batch_eval are available from previous steps\n",
        "# and `ensemble_predict_slice_by_slice` was successfully executed in cell `vPSRC0eT_svq`\n",
        "\n",
        "if 'ensemble_pred_mask' in globals() and 'y_batch_eval' in globals():\n",
        "    # Calculate metrics for the Bagging ensemble\n",
        "    ensemble_bagging_dice = dice_score(ensemble_pred_mask, safe_unsqueeze_mask(y_batch_eval)).item()\n",
        "    ensemble_bagging_avd = absolute_volume_difference(ensemble_pred_mask, safe_unsqueeze_mask(y_batch_eval))\n",
        "    ensemble_bagging_f1 = lesion_wise_f1_score(ensemble_pred_mask, safe_unsqueeze_mask(y_batch_eval))\n",
        "\n",
        "    bagging_results_df = pd.DataFrame({\n",
        "        'model_name': ['Bagging Ensemble'],\n",
        "        'avg_dice': [ensemble_bagging_dice],\n",
        "        'avg_avd': [ensemble_bagging_avd],\n",
        "        'avg_f1': [ensemble_bagging_f1]\n",
        "    })\n",
        "\n",
        "    print(\"\\nBagging Ensemble Performance:\")\n",
        "    display(bagging_results_df)\n",
        "\n",
        "    # Store metrics for overall summary later\n",
        "    all_results[\"Bagging Ensemble\"] = bagging_results_df.set_index('model_name').to_dict('index')\n",
        "else:\n",
        "    print(\"Bagging ensemble prediction or ground truth not found. Please ensure previous steps ran successfully.\")"
      ],
      "id": "f92ab9c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08a113ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires detailing experimental results. I need to calculate and display the performance metrics for the Boosting ensemble and store these results for later summarization. I will calculate Dice, AVD, and F1 for the Boosting ensemble prediction that was already generated.\n",
        "\n"
      ],
      "id": "08a113ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9de1a6ef"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming final_boosting_mask and y_batch_bagging are available from previous steps\n",
        "\n",
        "if 'final_boosting_mask' in globals() and 'y_batch_bagging' in globals():\n",
        "    # Calculate metrics for the Boosting ensemble\n",
        "    boosting_ensemble_dice = dice_score(final_boosting_mask, safe_unsqueeze_mask(y_batch_bagging)).item()\n",
        "    boosting_ensemble_avd = absolute_volume_difference(final_boosting_mask, safe_unsqueeze_mask(y_batch_bagging))\n",
        "    boosting_ensemble_f1 = lesion_wise_f1_score(final_boosting_mask, safe_unsqueeze_mask(y_batch_bagging))\n",
        "\n",
        "    boosting_results_df = pd.DataFrame({\n",
        "        'model_name': ['Boosting Ensemble'],\n",
        "        'avg_dice': [boosting_ensemble_dice],\n",
        "        'avg_avd': [boosting_ensemble_avd],\n",
        "        'avg_f1': [boosting_ensemble_f1]\n",
        "    })\n",
        "\n",
        "    print(\"\\nBoosting Ensemble Performance:\")\n",
        "    display(boosting_results_df)\n",
        "\n",
        "    # Store metrics for overall summary later\n",
        "    all_results[\"Boosting Ensemble\"] = boosting_results_df.set_index('model_name').to_dict('index')\n",
        "else:\n",
        "    print(\"Boosting ensemble prediction or ground truth not found. Please ensure previous steps ran successfully.\")"
      ],
      "id": "9de1a6ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bc7d493"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires detailing experimental results. I need to calculate and display the performance metrics for the Stacking ensemble and store these results for later summarization. I will calculate Dice, AVD, and F1 for the Stacking ensemble prediction.\n",
        "\n"
      ],
      "id": "4bc7d493"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88573ba5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming final_stacking_mask and y_batch_stacking are available from previous steps\n",
        "\n",
        "if 'final_stacking_mask' in globals() and 'y_batch_stacking' in globals():\n",
        "    # Calculate metrics for the Stacking ensemble\n",
        "    stacking_ensemble_dice = dice_score(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking)).item()\n",
        "    stacking_ensemble_avd = absolute_volume_difference(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking))\n",
        "    stacking_ensemble_f1 = lesion_wise_f1_score(final_stacking_mask, safe_unsqueeze_mask(y_batch_stacking))\n",
        "\n",
        "    stacking_results_df = pd.DataFrame({\n",
        "        'model_name': ['Stacking Ensemble'],\n",
        "        'avg_dice': [stacking_ensemble_dice],\n",
        "        'avg_avd': [stacking_ensemble_avd],\n",
        "        'avg_f1': [stacking_ensemble_f1]\n",
        "    })\n",
        "\n",
        "    print(\"\\nStacking Ensemble Performance:\")\n",
        "    display(stacking_results_df)\n",
        "\n",
        "    # Store metrics for overall summary later\n",
        "    all_results[\"Stacking Ensemble\"] = stacking_results_df.set_index('model_name').to_dict('index')\n",
        "else:\n",
        "    print(\"Stacking ensemble prediction or ground truth not found. Please ensure previous steps ran successfully.\")"
      ],
      "id": "88573ba5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1547bdb6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"\\n--- Overall Model Performance Summary ---\")\n",
        "\n",
        "# Convert the nested dictionary into a list of dictionaries for DataFrame creation\n",
        "summary_data = []\n",
        "\n",
        "# Individual Models\n",
        "if \"Individual Models\" in all_results:\n",
        "    for model_name, metrics in all_results[\"Individual Models\"].items():\n",
        "        summary_data.append({\"model_type\": \"Individual Model\", \"model_name\": model_name, **metrics})\n",
        "\n",
        "# Bagging Ensemble\n",
        "if \"Bagging Ensemble\" in all_results:\n",
        "    summary_data.append({\"model_type\": \"Ensemble\", \"model_name\": \"Bagging Ensemble\", **all_results[\"Bagging Ensemble\"][\"Bagging Ensemble\"]})\n",
        "\n",
        "# Boosting Ensemble\n",
        "if \"Boosting Ensemble\" in all_results:\n",
        "    summary_data.append({\"model_type\": \"Ensemble\", \"model_name\": \"Boosting Ensemble\", **all_results[\"Boosting Ensemble\"][\"Boosting Ensemble\"]})\n",
        "\n",
        "# Stacking Ensemble\n",
        "if \"Stacking Ensemble\" in all_results:\n",
        "    summary_data.append({\"model_type\": \"Ensemble\", \"model_name\": \"Stacking Ensemble\", **all_results[\"Stacking Ensemble\"][\"Stacking Ensemble\"]})\n",
        "\n",
        "# Create the summary DataFrame\n",
        "overall_summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Reorder columns for better readability\n",
        "overall_summary_df = overall_summary_df[[\"model_type\", \"model_name\", \"avg_dice\", \"avg_avd\", \"avg_f1\"]]\n",
        "\n",
        "# Display the summary table\n",
        "display(overall_summary_df)\n",
        "\n",
        "print(\"\\nThis table provides a comprehensive overview of the performance of individual models and different ensemble strategies.\")"
      ],
      "id": "1547bdb6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "gpuType": "G4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}